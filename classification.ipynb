{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8b4cca-6768-4ed1-9505-c95264c3176e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test_images is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfelin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mImage classification\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmnist_compressed.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m X_test, y_test, X_train, y_train \u001b[38;5;241m=\u001b[39m  \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_labels\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_images\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (60000, 28, 56)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert to TensorFlow Dataset\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:263\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a file in the archive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test_images is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten\n",
    "\n",
    "# Load data\n",
    "data = np.load(r'C:\\Users\\felin\\Downloads\\Image classification\\mnist_compressed.zip')\n",
    "X_test, y_test, X_train, y_train =  data['test_images'], data['test_labels'], data['train_images'], data['train_labels']\n",
    "print(X_train.shape)  # (60000, 28, 56)\n",
    "\n",
    "# Convert to TensorFlow Dataset\n",
    "test_full_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10)\n",
    "\n",
    "# Function to display an image\n",
    "def show_img(x, y):\n",
    "    plt.gray()\n",
    "    plt.title(str(y))\n",
    "    plt.imshow(x)\n",
    "\n",
    "# Function to divide train dataset into train and validation\n",
    "def divide_into_train_and_val(train_dataset_original, train_ratio=0.8, val_ratio=0.2):\n",
    "    DATASET_SIZE = len(train_dataset_original)\n",
    "\n",
    "    train_dataset = train_dataset_original.take(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "        (\n",
    "            tf.reshape(x , (28 , 56 , 1)) \n",
    "            , y\n",
    "        )\n",
    "    ).batch(32)\n",
    "\n",
    "    val_dataset = train_dataset_original.skip(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "        (\n",
    "            tf.reshape(x , (28 , 56 , 1)) \n",
    "            , y\n",
    "        )\n",
    "    ).batch(32)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_dataset , val_dataset = divide_into_train_and_val(train)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Conv2D(filters=8, kernel_size=(3, 3), strides=1, padding='same', activation='relu', input_shape=(28, 56, 1)),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=20)\n",
    "\n",
    "# Test on a specific image\n",
    "x = X_test[69]\n",
    "y = y_test[69]\n",
    "show_img(x, y)\n",
    "x = x.reshape(1, 28, 56, 1)\n",
    "predictions = model.predict(x, verbose=0)\n",
    "\n",
    "# Get predicted class and confidence\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "confidence = predictions[0][predicted_class]\n",
    "print('class =', predicted_class, 'conf=' , confidence)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('./mnist_trained_weights.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357edfd-4f90-409a-9a3c-0fc97b7bf00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's go through the code block by block to explain how it works:\n",
    "\n",
    "### 1. **Importing Libraries**\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Dense, Flatten\n",
    "```\n",
    "- **NumPy**: A library for handling large, multi-dimensional arrays and matrices, which is essential for numerical computation.\n",
    "- **Pandas**: This library is generally used for data manipulation and analysis, but it's not actively used in this particular code.\n",
    "- **Matplotlib**: A plotting library to visualize images and graphs.\n",
    "- **TensorFlow**: A deep learning framework, which is used here to build and train a neural network.\n",
    "- **Keras**: A high-level API for building neural networks within TensorFlow. It's used here to define the layers of the CNN model.\n",
    "\n",
    "### 2. **Loading Data**\n",
    "```python\n",
    "data = np.load(r'C:\\Users\\felin\\Downloads\\Image classification\\mnist_compressed.zip')\n",
    "X_test, y_test, X_train, y_train =  data['test_images'], data['test_labels'], data['train_images'], data['train_labels']\n",
    "print(X_train.shape)  # (60000, 28, 56)\n",
    "```\n",
    "- The `np.load()` function is used to load the `.npz` compressed file which contains the MNIST dataset (images and labels). This dataset is pre-split into training and testing data.\n",
    "- **X_train** and **X_test** contain the image data.\n",
    "- **y_train** and **y_test** contain the labels (0-9 for each image, corresponding to the digits).\n",
    "- The shape of `X_train` is printed. It has 60,000 images, each of size `28x56`. This is slightly different from the standard MNIST dataset (`28x28`), indicating that the images may have been resized or pre-processed.\n",
    "\n",
    "### 3. **Converting Data to TensorFlow Dataset**\n",
    "```python\n",
    "test_full_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10)\n",
    "```\n",
    "- `tf.data.Dataset.from_tensor_slices` creates a TensorFlow Dataset from the NumPy arrays.\n",
    "  - **train**: This dataset is shuffled with a buffer size of `10`. The `shuffle()` method ensures that the training data is randomly shuffled during the training process, preventing overfitting.\n",
    "  - **test_full_dataset**: This contains the test data (`X_test` and `y_test`), which will be used later for evaluation.\n",
    "\n",
    "### 4. **Function to Display Images**\n",
    "```python\n",
    "def show_img(x, y):\n",
    "    plt.gray()\n",
    "    plt.title(str(y))\n",
    "    plt.imshow(x)\n",
    "```\n",
    "- This function displays an image (`x`) using `matplotlib` with the title set to the label (`y`).\n",
    "- `plt.gray()` ensures the image is displayed in grayscale.\n",
    "- `plt.imshow(x)` displays the image.\n",
    "\n",
    "### 5. **Splitting Train Dataset into Train and Validation**\n",
    "```python\n",
    "def divide_into_train_and_val(train_dataset_original, train_ratio=0.8, val_ratio=0.2):\n",
    "    DATASET_SIZE = len(train_dataset_original)\n",
    "\n",
    "    train_dataset = train_dataset_original.take(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "        (\n",
    "            tf.reshape(x , (28 , 56 , 1)) \n",
    "            , y\n",
    "        )\n",
    "    ).batch(32)\n",
    "\n",
    "    val_dataset = train_dataset_original.skip(int(train_ratio * DATASET_SIZE)).map(lambda x, y: \n",
    "        (\n",
    "            tf.reshape(x , (28 , 56 , 1)) \n",
    "            , y\n",
    "        )\n",
    "    ).batch(32)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "```\n",
    "- This function splits the original training dataset into a training subset (80%) and a validation subset (20%).\n",
    "- `train_dataset_original.take()` is used to take the first 80% of the data for training.\n",
    "- `train_dataset_original.skip()` is used to skip the first 80% of the data and use the remaining 20% for validation.\n",
    "- `tf.reshape(x, (28, 56, 1))` reshapes the image data to have one channel (grayscale), which is needed for input into the CNN.\n",
    "- `.batch(32)` batches the data into mini-batches of 32 samples each.\n",
    "- The function returns the `train_dataset` and `val_dataset` for training and validation.\n",
    "\n",
    "### 6. **Building the Model**\n",
    "```python\n",
    "model = Sequential([\n",
    "    Conv2D(filters=8, kernel_size=(3, 3), strides=1, padding='same', activation='relu', input_shape=(28, 56, 1)),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "```\n",
    "- **Sequential**: A linear stack of layers.\n",
    "- **Conv2D**: A convolutional layer that applies 2D convolution to the input image. It has filters (kernels) of size `3x3` that slide across the image to extract features.\n",
    "  - The first convolution layer uses 8 filters and a `relu` activation function.\n",
    "  - The second convolution layer uses 16 filters and a `relu` activation function.\n",
    "  - `padding='same'` ensures that the output image dimensions are the same as the input for each convolutional layer.\n",
    "- **MaxPool2D**: A max pooling layer that reduces the dimensions of the output from the convolution layers, making the model more efficient.\n",
    "  - The pool size is `2x2`, and the stride is 2, which means it reduces the dimensions by half.\n",
    "- **Flatten**: Flattens the 2D output into a 1D vector to feed into the dense layer.\n",
    "- **Dense**: A fully connected layer with 100 units and a `softmax` activation function to output probabilities for each class (0-9).\n",
    "\n",
    "### 7. **Compiling the Model**\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "- **optimizer='adam'**: Adam is an adaptive learning rate optimization algorithm.\n",
    "- **loss='sparse_categorical_crossentropy'**: This is the loss function used for multi-class classification problems where labels are provided as integers (not one-hot encoded).\n",
    "- **metrics=['accuracy']**: The model will track the accuracy metric during training.\n",
    "\n",
    "### 8. **Training the Model**\n",
    "```python\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=20)\n",
    "```\n",
    "- The model is trained for 20 epochs using the `train_dataset` and validated using the `val_dataset`.\n",
    "- `epochs=20` means the model will go through the entire training data 20 times.\n",
    "\n",
    "### 9. **Testing the Model with a Specific Image**\n",
    "```python\n",
    "x = X_test[69]\n",
    "y = y_test[69]\n",
    "show_img(x, y)\n",
    "x = x.reshape(1, 28, 56, 1)\n",
    "predictions = model.predict(x, verbose=0)\n",
    "\n",
    "# Get predicted class and confidence\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "confidence = predictions[0][predicted_class]\n",
    "print('class =', predicted_class, 'conf=' , confidence)\n",
    "```\n",
    "- This tests the trained model on a specific test image (index 69).\n",
    "- The image `x` is reshaped to match the input shape `(28, 56, 1)`.\n",
    "- `model.predict()` is used to get the predicted class probabilities.\n",
    "- `np.argmax(predictions[0])` gets the class with the highest probability.\n",
    "- `predictions[0][predicted_class]` gives the confidence level of the prediction.\n",
    "\n",
    "### 10. **Saving the Model**\n",
    "```python\n",
    "model.save('./mnist_trained_weights.keras')\n",
    "```\n",
    "- The trained model is saved in the `mnist_trained_weights.keras` file so that it can be loaded and used later without having to retrain it.\n",
    "\n",
    "### Summary\n",
    "This code performs the following steps:\n",
    "1. Loads and processes the MNIST dataset.\n",
    "2. Defines a Convolutional Neural Network (CNN) with 2 convolution layers followed by max pooling, and a fully connected layer at the end.\n",
    "3. Compiles and trains the model using the Adam optimizer and sparse categorical cross-entropy loss.\n",
    "4. Evaluates the model's performance on test data and saves the trained model for future use.\n",
    "\n",
    "Let me know if you need further clarifications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
